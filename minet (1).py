# -*- coding: utf-8 -*-
"""MINet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gZ5jfez0GN6v6WqGr2I7cnnREr6aUOuL

# **Preprocessing**

Importing All Necessary Libraries
"""

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
from nltk.corpus import wordnet
nltk.download('averaged_perceptron_tagger')
import copy
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
!pip install pycaret
import pycaret
!pip install streamlit
import streamlit as st
!pip install contractions
import contractions

"""Importing Dataset"""

from google.colab import drive
drive.mount('/content/drive')

dataset = []

import os
import collections
import pandas as pd

cnt=0
for dp, dn, files in os.walk('/content/drive/MyDrive/TRANSCRIPTSFINAL'):
    cnt=cnt+1
    for f in files:
        if (f.endswith(".txt")): # if file has extension .txt, add it to array
            dataset.append(os.path.join(dp, f))

dataset.sort(key=str.lower)

textarray = []

for item in dataset:
    f = open(item, 'r')
    file_contents = f.read()
    textarray.append(file_contents)
    f.close()

"""Contraction Expansion"""

contractionsarray = []

for item in textarray:
  r = contractions.fix(item)
  contractionsarray.append(r)

"""Lowercase All"""

# lower all
lowerarray = []

for item in contractionsarray:
  r = item.lower()
  lowerarray.append(r)

"""Word Tokenization and Stopword Removal"""

# word tokenize + stopword removal
stopwords = nltk.corpus.stopwords.words('english')
removed = ['c', ':', 't', '.', '?', '!', ',']
stopwords.extend(removed)

tokenizeandstoparray = []

for item in lowerarray:
  r = nltk.word_tokenize(item)
  r2 = [word for word in r if word not in stopwords]
  tokenizeandstoparray.append(r2)

"""Stemming Words"""

#stemmingwords
ps = PorterStemmer()
templist = []
stemmedarray = []

for item in tokenizeandstoparray:
  for word in item:
    r = ps.stem(word)
    templist.append(r)
  stemmedarray.append(copy.deepcopy(templist))
  templist.clear()

"""Lemmatizing"""

# lemmatize + pos tag
def get_wordnet_pos(word):
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)

lemmatizer = WordNetLemmatizer()

lemmatizearray = []
templist = []

for item in tokenizeandstoparray:
  for word in item:
    r = ((lemmatizer.lemmatize(word, get_wordnet_pos(word))))
    templist.append(r)
  lemmatizearray.append(copy.deepcopy(templist))
  templist.clear()

"""# **Text Analysis**

## Word Count
"""

# word count with len.split
highwordcount = []

for item in lowerarray[0:154]:
  highwordcount.append(len(item.split()))

lowwordcount = []

for item in lowerarray[154:]:
  lowwordcount.append(len(item.split()))

# graphs
import matplotlib.pyplot as plt

highnumbers = list(range(0,154))
lownumbers = list(range(155,259))

plt.plot(highnumbers, highwordcount)
plt.plot(lownumbers, lowwordcount)
plt.xlabel("Document Number")
plt.ylabel("Word Count")
plt.show()

# loess for trendline
import pandas as pd
df = pd.DataFrame ({"Document Number" : highnumbers,
                    "Word Count": highwordcount})
df1 = pd.DataFrame ({"Document Number" : lownumbers,
                    "Word Count": lowwordcount})

import numpy as np

def loc_eval(x, b):
    loc_est = 0
    for i in enumerate(b): loc_est+=i[1]*(x**i[0])
    return(loc_est)

def loess(xvals, yvals, data, alpha, poly_degree=1):
    all_data = sorted(zip(data[xvals].tolist(), data[yvals].tolist()), key=lambda x: x[0])
    xvals, yvals = zip(*all_data)
    evalDF = pd.DataFrame(columns=['v','g'])
    n = len(xvals)
    m = n + 1
    q = int(np.floor(n * alpha) if alpha <= 1.0 else n)
    avg_interval = ((max(xvals)-min(xvals))/len(xvals))
    v_lb = min(xvals)-(.5*avg_interval)
    v_ub = (max(xvals)+(.5*avg_interval))
    v = enumerate(np.linspace(start=v_lb, stop=v_ub, num=m), start=1)
    xcols = [np.ones_like(xvals)]
    for j in range(1, (poly_degree + 1)):
        xcols.append([i ** j for i in xvals])
    X = np.vstack(xcols).T
    for i in v:
        iterpos = i[0]
        iterval = i[1]
        iterdists = sorted([(j, np.abs(j-iterval)) for j in xvals], key=lambda x: x[1])
        _, raw_dists = zip(*iterdists)
        scale_fact = raw_dists[q-1]
        scaled_dists = [(j[0],(j[1]/scale_fact)) for j in iterdists]
        weights = [(j[0],((1-np.abs(j[1]**3))**3 if j[1]<=1 else 0)) for j in scaled_dists]
        _, weights      = zip(*sorted(weights,     key=lambda x: x[0]))
        _, raw_dists    = zip(*sorted(iterdists,   key=lambda x: x[0]))
        _, scaled_dists = zip(*sorted(scaled_dists,key=lambda x: x[0]))
        W         = np.diag(weights)
        b         = np.linalg.inv(X.T @ W @ X) @ (X.T @ W @ yvals)
        local_est = loc_eval(iterval, b)
        iterDF2   = pd.DataFrame({
                       'v'  :[iterval],
                       'g'  :[local_est]
                       })
        evalDF = pd.concat([evalDF, iterDF2])
    evalDF = evalDF[['v','g']]
    return(evalDF)


evalDF = loess("Document Number", "Word Count", data = df, alpha=0.7, poly_degree=2)
evalDF2 = loess("Document Number", "Word Count", data=df1, alpha=0.7, poly_degree=2)

fig = plt.figure()
ax1 = fig.add_subplot(111)
ax1.scatter(df["Document Number"], df["Word Count"], color="grey", marker="o", s=5, label="_nolegend_")
ax1.plot(evalDF['v'], evalDF['g'], color='red', linewidth= 3, label="High Quality")
ax1 = fig.add_subplot(111)
ax1.scatter(df1["Document Number"], df1["Word Count"], color="grey", marker="o", s=5, label="_nolegend_")
ax1.plot(evalDF2['v'], evalDF2['g'], color='turquoise', linewidth= 3, label="Low Quality")
plt.title('Word Count')
plt.legend()
plt.tight_layout()
plt.show()

"""## Sentiment Analysis"""

!pip install nltk==3.3
import nltk

positivedataset = []
negativedataset = []
for item in lowerarray[0:154]:
  positivedataset.append(item)
for item in lowerarray[154:]:
  negativedataset.append(item)

"""Sentiment - Positive

"""

# Install; note that the prefix "!" is not needed if you are running in a terminal
!pip install stanza

# Import the package
import stanza

# Download an English model into the default directory
print("Downloading English model...")
stanza.download('en')

# Build an English pipeline, with all processors by default
print("Building an English pipeline...")
en_nlp = stanza.Pipeline('en')

sentimentposit1 = []
for item in positivedataset:
  lines = item.splitlines()
  sentimentposit =[]
  for item in lines:
    sentimentposit3 = []
    doc = en_nlp(item)
    for i, sentence in enumerate(doc.sentences):
      r =  (sentence.sentiment)
      sentimentposit3.append(r)
    sentimentposit.append(sentimentposit3)
  sentimentposit1.append(sentimentposit)

finalhistogramvalues = []
for item in sentimentposit1:
  for item in item:
    finalhistogramvalues.append(item)

finalhistovals= []
for item in finalhistogramvalues:
  for item in item:
    finalhistovals.append(item)

import numpy as np
import matplotlib.pyplot as plt

def make_hist(ax, x, bins=None, binlabels=None, width=0.85, extra_x=1, extra_y=4,
              text_offset=0.3, title=r"Frequency diagram",
              xlabel="Values", ylabel="Frequency"):
    if bins is None:
        xmax = max(x)+extra_x
        bins = range(xmax+1)
    if binlabels is None:
        if np.issubdtype(np.asarray(x).dtype, np.integer):
            binlabels = [str(bins[i]) if bins[i+1]-bins[i] == 1 else
                         '{}-{}'.format(bins[i], bins[i+1]-1)
                         for i in range(len(bins)-1)]
        else:
            binlabels = [str(bins[i]) if bins[i+1]-bins[i] == 1 else
                         '{}-{}'.format(*bins[i:i+2])
                         for i in range(len(bins)-1)]
        if bins[-1] == np.inf:
            binlabels[-1] = '{}+'.format(bins[-2])
    n, bins = np.histogram(x, bins=bins)
    patches = ax.bar(range(len(n)), n, align='center', width=width)
    ymax = max(n)+extra_y

    ax.set_xticks(range(len(binlabels)))
    ax.set_xticklabels(binlabels)

    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    ax.set_ylim(0, 7000)
    ax.grid(True, axis='y')
    # http://stackoverflow.com/a/28720127/190597 (peeol)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['bottom'].set_visible(False)
    ax.spines['left'].set_visible(False)
    # http://stackoverflow.com/a/11417222/190597 (gcalmettes)
    ax.xaxis.set_ticks_position('none')
    ax.yaxis.set_ticks_position('none')
    autolabel(patches, text_offset)

def autolabel(rects, shift=0.3):
    """
    http://matplotlib.org/1.2.1/examples/pylab_examples/barchart_demo.html
    """
    # attach some text labels
    for rect in rects:
        height = rect.get_height()
        if height > 0:
            plt.text(rect.get_x()+rect.get_width()/2., height+shift, '%d'%int(height),
                     ha='center', va='bottom')


fig, ax = plt.subplots(figsize=(14,6))
# make_hist(ax, x)
# make_hist(ax, [1,1,1,0,0,0], extra_y=1, text_offset=0.1)
make_hist(ax, finalhistovals, bins=list(range(4)), extra_y=6)
plt.show()

"""Sentiment - Negative"""

sentimentnegat1 = []
for item in negativedataset:
  lines = item.splitlines()
  sentimentnegat =[]
  for item in lines:
    sentimentnegat3 = []
    doc = en_nlp(item)
    for i, sentence in enumerate(doc.sentences):
      r =  (sentence.sentiment)
      sentimentnegat3.append(r)
    sentimentnegat.append(sentimentnegat3)
  sentimentnegat1.append(sentimentnegat)

finalhistogramvalues2 = []
for item in sentimentnegat1:
  for item in item:
    finalhistogramvalues2.append(item)

finalhistovals3= []
for item in finalhistogramvalues2:
  for item in item:
    finalhistovals3.append(item)

import numpy as np
import matplotlib.pyplot as plt

def make_hist(ax, x, bins=None, binlabels=None, width=0.85, extra_x=1, extra_y=4,
              text_offset=0.3, title=r"Frequency diagram",
              xlabel="Values", ylabel="Frequency"):
    if bins is None:
        xmax = max(x)+extra_x
        bins = range(xmax+1)
    if binlabels is None:
        if np.issubdtype(np.asarray(x).dtype, np.integer):
            binlabels = [str(bins[i]) if bins[i+1]-bins[i] == 1 else
                         '{}-{}'.format(bins[i], bins[i+1]-1)
                         for i in range(len(bins)-1)]
        else:
            binlabels = [str(bins[i]) if bins[i+1]-bins[i] == 1 else
                         '{}-{}'.format(*bins[i:i+2])
                         for i in range(len(bins)-1)]
        if bins[-1] == np.inf:
            binlabels[-1] = '{}+'.format(bins[-2])
    n, bins = np.histogram(x, bins=bins)
    patches = ax.bar(range(len(n)), n, align='center', width=width)
    ymax = max(n)+extra_y

    ax.set_xticks(range(len(binlabels)))
    ax.set_xticklabels(binlabels)

    ax.set_title(title)
    ax.set_xlabel(xlabel)
    ax.set_ylabel(ylabel)
    ax.set_ylim(0, 7000)
    ax.grid(True, axis='y')
    # http://stackoverflow.com/a/28720127/190597 (peeol)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['bottom'].set_visible(False)
    ax.spines['left'].set_visible(False)
    # http://stackoverflow.com/a/11417222/190597 (gcalmettes)
    ax.xaxis.set_ticks_position('none')
    ax.yaxis.set_ticks_position('none')
    autolabel(patches, text_offset)

def autolabel(rects, shift=0.3):
    """
    http://matplotlib.org/1.2.1/examples/pylab_examples/barchart_demo.html
    """
    # attach some text labels
    for rect in rects:
        height = rect.get_height()
        if height > 0:
            plt.text(rect.get_x()+rect.get_width()/2., height+shift, '%d'%int(height),
                     ha='center', va='bottom')


fig, ax = plt.subplots(figsize=(14,6))
# make_hist(ax, x)
# make_hist(ax, [1,1,1,0,0,0], extra_y=1, text_offset=0.1)
make_hist(ax, finalhistovals3, bins=list(range(4)), extra_y=6)
plt.show()

"""Frequency

## Word Length Between Counselor and Patients
"""

# create counselor and patient dataset
from nltk import tokenize
from nltk.tokenize.treebank import TreebankWordDetokenizer

patientposit =[]
papositlist = []

# patient positive
for item in positivedataset:
  lines = item.splitlines()
  patientposit =[]
  for item in lines:
    if item.startswith("c:	"):
      patientposit.append(item)
  papositlist.append(patientposit)
counselorposit = []
cpositlist = []

# counselor positive
for item in positivedataset:
  lines = item.splitlines()
  counselorposit =[]
  for item in lines:
    if item.startswith("t:	"):
      counselorposit.append(item)
  cpositlist.append(counselorposit)

# patient negative
patientnegat = []
panegitlist = []

for item in negativedataset:
  lines = item.splitlines()
  patientnegat =[]
  for item in lines:
    if item.startswith("c:	"):
      patientnegat.append(item)
  panegitlist.append(patientnegat)

# counselor negative
counselornegat = []
cnegitlist = []
for item in negativedataset:
  lines = item.splitlines()
  counselornegat =[]
  for item in lines:
    if item.startswith ("t:	"):
      counselornegat.append(item)
  cnegitlist.append(counselornegat)

positivecounselorwordcount = []
for item in cpositlist:
  for item in item:
    positivecounselorwordcount1 = ' '.join(map(str, item))
  positivecounselorwordcount.append(len(positivecounselorwordcount1))


negativecounselorwordcount = []
for item in cnegitlist:
  for item in item:
    negativecounselorwordcount1 = ' '.join(map(str,item))
  negativecounselorwordcount.append(len(negativecounselorwordcount1))

positivepatientwordcount = []
for item in papositlist:
  for item in item:
    positivepatientwordcount1 = ' '.join(map(str,item))
  positivepatientwordcount.append(len(positivepatientwordcount1))

negativepatientwordcount = []
for item in panegitlist:
  for item in item:
    negativepatientwordcount1 = ' '.join(map(str,item))
  negativepatientwordcount.append(len(negativepatientwordcount1))

# graphs
import matplotlib.pyplot as plt

highnumbers = list(range(0,154))
lownumbers = list(range(155,259))

plt.plot(highnumbers, positivecounselorwordcount, "red")
plt.plot(lownumbers, negativecounselorwordcount, "pink")
plt.xlabel("Document Number")
plt.ylabel("Word Count")
plt.show()
plt.plot(highnumbers, positivepatientwordcount, "blue")
plt.plot(lownumbers, negativepatientwordcount, "green")
plt.xlabel("Document Number")
plt.ylabel("Word Count")
plt.show()

# loess for trendline
import pandas as pd
df = pd.DataFrame ({"Document Number" : highnumbers,
                    "Word Count": positivecounselorwordcount})
df1 = pd.DataFrame ({"Document Number" : highnumbers,
                    "Word Count": positivepatientwordcount})
df2 = pd.DataFrame ({"Document Number" : lownumbers,
                    "Word Count": negativecounselorwordcount})
df3 = pd.DataFrame ({"Document Number" : lownumbers,
                    "Word Count": negativepatientwordcount})


import numpy as np

def loc_eval(x, b):
    loc_est = 0
    for i in enumerate(b): loc_est+=i[1]*(x**i[0])
    return(loc_est)

def loess(xvals, yvals, data, alpha, poly_degree=1):
    all_data = sorted(zip(data[xvals].tolist(), data[yvals].tolist()), key=lambda x: x[0])
    xvals, yvals = zip(*all_data)
    evalDF = pd.DataFrame(columns=['v','g'])
    n = len(xvals)
    m = n + 1
    q = int(np.floor(n * alpha) if alpha <= 1.0 else n)
    avg_interval = ((max(xvals)-min(xvals))/len(xvals))
    v_lb = min(xvals)-(.5*avg_interval)
    v_ub = (max(xvals)+(.5*avg_interval))
    v = enumerate(np.linspace(start=v_lb, stop=v_ub, num=m), start=1)
    xcols = [np.ones_like(xvals)]
    for j in range(1, (poly_degree + 1)):
        xcols.append([i ** j for i in xvals])
    X = np.vstack(xcols).T
    for i in v:
        iterpos = i[0]
        iterval = i[1]
        iterdists = sorted([(j, np.abs(j-iterval)) for j in xvals], key=lambda x: x[1])
        _, raw_dists = zip(*iterdists)
        scale_fact = raw_dists[q-1]
        scaled_dists = [(j[0],(j[1]/scale_fact)) for j in iterdists]
        weights = [(j[0],((1-np.abs(j[1]**3))**3 if j[1]<=1 else 0)) for j in scaled_dists]
        _, weights      = zip(*sorted(weights,     key=lambda x: x[0]))
        _, raw_dists    = zip(*sorted(iterdists,   key=lambda x: x[0]))
        _, scaled_dists = zip(*sorted(scaled_dists,key=lambda x: x[0]))
        W         = np.diag(weights)
        b         = np.linalg.inv(X.T @ W @ X) @ (X.T @ W @ yvals)
        local_est = loc_eval(iterval, b)
        iterDF2   = pd.DataFrame({
                       'v'  :[iterval],
                       'g'  :[local_est]
                       })
        evalDF = pd.concat([evalDF, iterDF2])
    evalDF = evalDF[['v','g']]
    return(evalDF)


evalDF = loess("Document Number", "Word Count", data = df, alpha=0.7, poly_degree=2)
evalDF2 = loess("Document Number", "Word Count", data=df1, alpha=0.7, poly_degree=2)
evalDF3 = loess("Document Number", "Word Count", data=df2, alpha=0.7, poly_degree=2)
evalDF4 = loess("Document Number", "Word Count", data=df3, alpha=0.7, poly_degree=2)

fig = plt.figure()
ax1 = fig.add_subplot(111)
ax1.scatter(df["Document Number"], df["Word Count"], color="grey", marker="o", s=5, label="_nolegend_")
ax1.plot(evalDF['v'], evalDF['g'], color='red', linewidth= 3, label="High Quality Counselor")
ax1 = fig.add_subplot(111)
ax1.scatter(df2["Document Number"], df2["Word Count"], color="grey", marker="o", s=5, label="_nolegend_")
ax1.plot(evalDF3['v'], evalDF3['g'], color='turquoise', linewidth= 3, label="Low Quality Counselor")
plt.title('N = 10, alpha = 0.7, polynomial degree = 2')
plt.legend()
plt.tight_layout()
plt.show()

fig = plt.figure()
ax1 = fig.add_subplot(111)
ax1.scatter(df1["Document Number"], df1["Word Count"], color="grey", marker="o", s=5, label="_nolegend_")
ax1.plot(evalDF2['v'], evalDF2['g'], color='red', linewidth= 3, label="High Quality Patient")
ax1 = fig.add_subplot(111)
ax1.scatter(df3["Document Number"], df3["Word Count"], color="grey", marker="o", s=5, label="_nolegend_")
ax1.plot(evalDF4['v'], evalDF4['g'], color='turquoise', linewidth= 3, label="Low Quality Patient")
plt.title('N = 10, alpha = 0.7, polynomial degree = 2')
plt.legend()
plt.tight_layout()
plt.show()

highratio = [i / j for i, j in zip(positivecounselorwordcount, positivepatientwordcount)]
lowratio = [i / j for i, j in zip(negativecounselorwordcount, negativepatientwordcount)]

plt.plot(highnumbers, highratio, "red")
plt.plot(lownumbers, lowratio, "pink")
plt.xlabel("Document Number")
plt.ylabel("Counselor-Patient Ratio")
plt.show()

# loess for trendline
import pandas as pd
df = pd.DataFrame ({"Document Number" : highnumbers,
                    "Counselor-Patient Ratio": highratio})
df2 = pd.DataFrame ({"Document Number" : lownumbers,
                    "Counselor-Patient Ratio": lowratio})


import numpy as np

def loc_eval(x, b):
    loc_est = 0
    for i in enumerate(b): loc_est+=i[1]*(x**i[0])
    return(loc_est)

def loess(xvals, yvals, data, alpha, poly_degree=1):
    all_data = sorted(zip(data[xvals].tolist(), data[yvals].tolist()), key=lambda x: x[0])
    xvals, yvals = zip(*all_data)
    evalDF = pd.DataFrame(columns=['v','g'])
    n = len(xvals)
    m = n + 1
    q = int(np.floor(n * alpha) if alpha <= 1.0 else n)
    avg_interval = ((max(xvals)-min(xvals))/len(xvals))
    v_lb = min(xvals)-(.5*avg_interval)
    v_ub = (max(xvals)+(.5*avg_interval))
    v = enumerate(np.linspace(start=v_lb, stop=v_ub, num=m), start=1)
    xcols = [np.ones_like(xvals)]
    for j in range(1, (poly_degree + 1)):
        xcols.append([i ** j for i in xvals])
    X = np.vstack(xcols).T
    for i in v:
        iterpos = i[0]
        iterval = i[1]
        iterdists = sorted([(j, np.abs(j-iterval)) for j in xvals], key=lambda x: x[1])
        _, raw_dists = zip(*iterdists)
        scale_fact = raw_dists[q-1]
        scaled_dists = [(j[0],(j[1]/scale_fact)) for j in iterdists]
        weights = [(j[0],((1-np.abs(j[1]**3))**3 if j[1]<=1 else 0)) for j in scaled_dists]
        _, weights      = zip(*sorted(weights,     key=lambda x: x[0]))
        _, raw_dists    = zip(*sorted(iterdists,   key=lambda x: x[0]))
        _, scaled_dists = zip(*sorted(scaled_dists,key=lambda x: x[0]))
        W         = np.diag(weights)
        b         = np.linalg.inv(X.T @ W @ X) @ (X.T @ W @ yvals)
        local_est = loc_eval(iterval, b)
        iterDF2   = pd.DataFrame({
                       'v'  :[iterval],
                       'g'  :[local_est]
                       })
        evalDF = pd.concat([evalDF, iterDF2])
    evalDF = evalDF[['v','g']]
    return(evalDF)


evalDF = loess("Document Number", "Counselor-Patient Ratio", data = df, alpha=0.7, poly_degree=2)
evalDF3 = loess("Document Number", "Counselor-Patient Ratio", data=df2, alpha=0.7, poly_degree=2)


fig = plt.figure()
ax1 = fig.add_subplot(111)
ax1.scatter(df["Document Number"], df["Counselor-Patient Ratio"], color="grey", marker="o", s=5, label="_nolegend_")
ax1.plot(evalDF['v'], evalDF['g'], color='red', linewidth= 3, label="High Quality Counselor")
ax1 = fig.add_subplot(111)
ax1.scatter(df2["Document Number"], df2["Counselor-Patient Ratio"], color="grey", marker="o", s=5, label="_nolegend_")
ax1.plot(evalDF3['v'], evalDF3['g'], color='turquoise', linewidth= 3, label="Low Quality Counselor")
plt.title('Counselor-to-Patient Word Ratios')
plt.legend()
plt.tight_layout()
plt.show()



"""## Word Similarity between Positive and Negative"""

def get_jaccard_sim(str1, str2):
    a = set(str1.split())
    b = set(str2.split())
    c = a.intersection(b)
    return float(len(c)) / (len(a) + len(b) - len(c))

jpositsimilarity = []
for c,p in zip(cpositlist, papositlist):
  jaccardsimposit = get_jaccard_sim(str(c),str(p))
  jpositsimilarity.append(jaccardsimposit)

jnegitsimilarity = []
for c,p in zip(cnegitlist, panegitlist):
  jaccardsimnegit = get_jaccard_sim(str(c),str(p))
  jnegitsimilarity.append(jaccardsimnegit)

plt.plot(highnumbers, jpositsimilarity, "aqua")
plt.plot(lownumbers, jnegitsimilarity, "pink")
plt.xlabel("Document Number")
plt.ylabel("Jaccard Similarity")
plt.show()

# loess for trendline
import pandas as pd
df = pd.DataFrame ({"Document Number" : highnumbers,
                    "Jaccard Similarity": jpositsimilarity})
df2 = pd.DataFrame ({"Document Number" : lownumbers,
                    "Jaccard Similarity": jnegitsimilarity})


import numpy as np

def loc_eval(x, b):
    loc_est = 0
    for i in enumerate(b): loc_est+=i[1]*(x**i[0])
    return(loc_est)

def loess(xvals, yvals, data, alpha, poly_degree=1):
    all_data = sorted(zip(data[xvals].tolist(), data[yvals].tolist()), key=lambda x: x[0])
    xvals, yvals = zip(*all_data)
    evalDF = pd.DataFrame(columns=['v','g'])
    n = len(xvals)
    m = n + 1
    q = int(np.floor(n * alpha) if alpha <= 1.0 else n)
    avg_interval = ((max(xvals)-min(xvals))/len(xvals))
    v_lb = min(xvals)-(.5*avg_interval)
    v_ub = (max(xvals)+(.5*avg_interval))
    v = enumerate(np.linspace(start=v_lb, stop=v_ub, num=m), start=1)
    xcols = [np.ones_like(xvals)]
    for j in range(1, (poly_degree + 1)):
        xcols.append([i ** j for i in xvals])
    X = np.vstack(xcols).T
    for i in v:
        iterpos = i[0]
        iterval = i[1]
        iterdists = sorted([(j, np.abs(j-iterval)) for j in xvals], key=lambda x: x[1])
        _, raw_dists = zip(*iterdists)
        scale_fact = raw_dists[q-1]
        scaled_dists = [(j[0],(j[1]/scale_fact)) for j in iterdists]
        weights = [(j[0],((1-np.abs(j[1]**3))**3 if j[1]<=1 else 0)) for j in scaled_dists]
        _, weights      = zip(*sorted(weights,     key=lambda x: x[0]))
        _, raw_dists    = zip(*sorted(iterdists,   key=lambda x: x[0]))
        _, scaled_dists = zip(*sorted(scaled_dists,key=lambda x: x[0]))
        W         = np.diag(weights)
        b         = np.linalg.inv(X.T @ W @ X) @ (X.T @ W @ yvals)
        local_est = loc_eval(iterval, b)
        iterDF2   = pd.DataFrame({
                       'v'  :[iterval],
                       'g'  :[local_est]
                       })
        evalDF = pd.concat([evalDF, iterDF2])
    evalDF = evalDF[['v','g']]
    return(evalDF)


evalDF = loess("Document Number", "Jaccard Similarity", data = df, alpha=0.7, poly_degree=2)
evalDF3 = loess("Document Number", "Jaccard Similarity", data=df2, alpha=0.7, poly_degree=2)


fig = plt.figure()
ax1 = fig.add_subplot(111)
ax1.scatter(df["Document Number"], df["Jaccard Similarity"], color="grey", marker="o", s=5, label="_nolegend_")
ax1.plot(evalDF['v'], evalDF['g'], color='red', linewidth= 3, label="High Quality Counselor")
ax1 = fig.add_subplot(111)
ax1.scatter(df2["Document Number"], df2["Jaccard Similarity"], color="grey", marker="o", s=5, label="_nolegend_")
ax1.plot(evalDF3['v'], evalDF3['g'], color='turquoise', linewidth= 3, label="Low Quality Counselor")
plt.title('Jaccard Similarity')
plt.legend()
plt.tight_layout()
plt.show()

"""## Topic Modeling with LDA"""

!pip install nltk.downloads('stopwords')
!pip install spacy

lemmatizepositive = []
lemmatizenegative = []

for item in lemmatizearray[0:154]:
  lemmatizepositive.append(item)
for item in lemmatizearray[154:]:
  lemmatizenegative.append(item)

# Commented out IPython magic to ensure Python compatibility.
from gensim import corpora, models

# list_of_list_of_tokens = [["a","b","c"], ["d","e","f"]]
# ["a","b","c"] are the tokens of document 1, ["d","e","f"] are the tokens of document 2...
dictionary_LDA = corpora.Dictionary(lemmatizepositive)
corpus = [dictionary_LDA.doc2bow(list_of_tokens) for list_of_tokens in lemmatizepositive]

num_topics = 6
# %time lda_model = models.LdaModel(corpus, num_topics=num_topics, \
                                  id2word=dictionary_LDA, \
                                  passes=4, alpha=[0.01]*num_topics, \
                                  eta=[0.01]*len(dictionary_LDA.keys()))

for i,topic in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=5):
    print(str(i)+": "+ topic)
    print()

from gensim.models import CoherenceModel
# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_model, texts=lemmatizepositive, dictionary=dictionary_LDA, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

# Commented out IPython magic to ensure Python compatibility.
from gensim import corpora, models

# list_of_list_of_tokens = [["a","b","c"], ["d","e","f"]]
# ["a","b","c"] are the tokens of document 1, ["d","e","f"] are the tokens of document 2...
dictionary_LDA = corpora.Dictionary(lemmatizenegative)
corpus = [dictionary_LDA.doc2bow(list_of_tokens) for list_of_tokens in lemmatizenegative]

num_topics = 6
# %time lda_model = models.LdaModel(corpus, num_topics=num_topics, \
                                  id2word=dictionary_LDA, \
                                  passes=4, alpha=[0.01]*num_topics, \
                                  eta=[0.01]*len(dictionary_LDA.keys()))



for i,topic in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=5):
    print(str(i)+": "+ topic)
    print()



from gensim.models import CoherenceModel
# Compute Coherence Score
coherence_model_lda = CoherenceModel(model=lda_model, texts=lemmatizenegative, dictionary=dictionary_LDA, coherence='c_v')
coherence_lda = coherence_model_lda.get_coherence()
print('\nCoherence Score: ', coherence_lda)

# supporting function
def compute_coherence_values(corpus, dictionary, k, a, b):

    lda_model = gensim.models.LdaMulticore(corpus=corpus,
                                           id2word=dictionary,
                                           num_topics=k,
                                           random_state=100,
                                           chunksize=100,
                                           passes=10,
                                           alpha=a,
                                           eta=b)

    coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')

    return coherence_model_lda.get_coherence()

"""# Model Building

## Document Numbers
"""

documentnumber = highnumbers + lownumbers

"""## Document Word Count"""

wordcount = highwordcount + lowwordcount

"""## Sentiment"""

import itertools

positivenegsentiment = []
positiveneusentiment = []
positivepossentiment = []
newsentimentposit1 = []

sentimentposit1[100] = [[1, 0], [1], [1], [1], [1], [1], [1, 1], [1], [1], [1], [2], [1], [0, 1, 1], [1], [0, 1], [0]]
sentimentposit1[138] = [[0], [0], [0], [0], [0], [0], [0], [0]]
for item in sentimentposit1:
  merged = list(itertools.chain(*item))
  newsentimentposit1.append(merged)

from collections import Counter
sentimentpositdict = []
for item in newsentimentposit1:
  a = Counter(item)
  b= dict(a)
  sentimentpositdict.append(b)
  searchkey1 = 0
  searchkey2 = 1
  searchkey3 = 2
  if searchkey1 in b.keys():
    positivenegsentiment.append((b[0])/(len(item)))
  else:
    positivenegsentiment.append('')
  if searchkey2 in b.keys():
    positiveneusentiment.append((b[1])/(len(item)))
  else:
    positiveneusentiment.append('')
  if searchkey3 in b.keys():
    positivepossentiment.append((b[2])/(len(item)))
  else:
    positivepossentiment.append('')

import itertools

negativenegsentiment = []
negativeneusentiment = []
negativepossentiment = []
newsentimentposit2 = []

for item in sentimentnegat1:
  merged = list(itertools.chain(*item))
  newsentimentposit2.append (merged)

from collections import Counter
sentimentnegatdict = []
for item in newsentimentposit2:
  asd = Counter(item)
  bwe = dict(asd)
  sentimentnegatdict.append(bwe)
  searchkey4 = 0
  searchkey5 = 1
  searchkey6 = 2
  if searchkey4 in bwe.keys():
    negativenegsentiment.append((bwe[0])/(len(item)))
  else:
    negativenegsentiment.append('')
  if searchkey5 in bwe.keys():
    negativeneusentiment.append((bwe[1])/(len(item)))
  else:
    negativeneusentiment.append('')
  if searchkey6 in bwe.keys():
    negativepossentiment.append((bwe[2])/(len(item)))
  else:
    negativepossentiment.append('')

positivepossentiment[2]=0
positivepossentiment[21]=0
positivepossentiment[22]=0
positivepossentiment[138]=0
positiveneusentiment[138] = 0
negativeneusentiment[59]=0
negativeneusentiment[74]=0
negativeneusentiment[81]=0
negativepossentiment[3] = 0
negativepossentiment[5] = 0
negativepossentiment[6] = 0
negativepossentiment[40] = 0
negativepossentiment[45] = 0
negativepossentiment[48] = 0
negativepossentiment[67] = 0
negativepossentiment[69] = 0
negativepossentiment[70] = 0
negativepossentiment[71] = 0
negativepossentiment[74] = 0
negativepossentiment[81] = 0
negativepossentiment[90] = 0

overallnegative = positivenegsentiment + negativenegsentiment
overallneutral = positiveneusentiment + negativeneusentiment
overallpositive = positivepossentiment + negativepossentiment

overallpositive[2] = 0
overallpositive[21] =0
overallpositive[22]=0
overallpositive[138]=0
overallpositive[157]=0
overallpositive[159]=0
overallpositive[160]=0
overallpositive[194]=0
overallpositive[199]=0
overallpositive[202]=0
overallpositive[221]=0
overallpositive[223]=0
overallpositive[224]=0
overallpositive[225]=0
overallpositive[228]=0
overallpositive[235]=0
overallpositive[160]=0
overallpositive[244]=0

overallneutral[138]=0
overallneutral[213]=0
overallneutral[228]=0
overallneutral[235]=0

# loess for trendline
import pandas as pd
df = pd.DataFrame ({"Document Number" : highnumbers,
                    "High-Quality Positive Sentiment": positivepossentiment})
df2 = pd.DataFrame ({"Document Number" : lownumbers,
                    "Low-Quality Positive Sentiment": negativepossentiment})


import numpy as np

def loc_eval(x, b):
    loc_est = 0
    for i in enumerate(b): loc_est+=i[1]*(x**i[0])
    return(loc_est)

def loess(xvals, yvals, data, alpha, poly_degree=1):
    all_data = sorted(zip(data[xvals].tolist(), data[yvals].tolist()), key=lambda x: x[0])
    xvals, yvals = zip(*all_data)
    evalDF = pd.DataFrame(columns=['v','g'])
    n = len(xvals)
    m = n + 1
    q = int(np.floor(n * alpha) if alpha <= 1.0 else n)
    avg_interval = ((max(xvals)-min(xvals))/len(xvals))
    v_lb = min(xvals)-(.5*avg_interval)
    v_ub = (max(xvals)+(.5*avg_interval))
    v = enumerate(np.linspace(start=v_lb, stop=v_ub, num=m), start=1)
    xcols = [np.ones_like(xvals)]
    for j in range(1, (poly_degree + 1)):
        xcols.append([i ** j for i in xvals])
    X = np.vstack(xcols).T
    for i in v:
        iterpos = i[0]
        iterval = i[1]
        iterdists = sorted([(j, np.abs(j-iterval)) for j in xvals], key=lambda x: x[1])
        _, raw_dists = zip(*iterdists)
        scale_fact = raw_dists[q-1]
        scaled_dists = [(j[0],(j[1]/scale_fact)) for j in iterdists]
        weights = [(j[0],((1-np.abs(j[1]**3))**3 if j[1]<=1 else 0)) for j in scaled_dists]
        _, weights      = zip(*sorted(weights,     key=lambda x: x[0]))
        _, raw_dists    = zip(*sorted(iterdists,   key=lambda x: x[0]))
        _, scaled_dists = zip(*sorted(scaled_dists,key=lambda x: x[0]))
        W         = np.diag(weights)
        b         = np.linalg.inv(X.T @ W @ X) @ (X.T @ W @ yvals)
        local_est = loc_eval(iterval, b)
        iterDF2   = pd.DataFrame({
                       'v'  :[iterval],
                       'g'  :[local_est]
                       })
        evalDF = pd.concat([evalDF, iterDF2])
    evalDF = evalDF[['v','g']]
    return(evalDF)


evalDF = loess("Document Number", "High-Quality Positive Sentiment", data = df, alpha=0.7, poly_degree=2)
evalDF3 = loess("Document Number", "Low-Quality Positive Sentiment", data=df2, alpha=0.7, poly_degree=2)


fig = plt.figure()
ax1 = fig.add_subplot(111)
ax1.scatter(df["Document Number"], df["High-Quality Positive Sentiment"], color="grey", marker="o", s=5, label="_nolegend_")
ax1.plot(evalDF['v'], evalDF['g'], color='red', linewidth= 3, label="High Quality Counselor")
ax1 = fig.add_subplot(111)
ax1.scatter(df2["Document Number"], df2["Low-Quality Positive Sentiment"], color="grey", marker="o", s=5, label="_nolegend_")
ax1.plot(evalDF3['v'], evalDF3['g'], color='turquoise', linewidth= 3, label="Low Quality Counselor")
plt.title('Frequency of Positive Sentiment')
plt.legend()
plt.tight_layout()
plt.show()

# loess for trendline
import pandas as pd
df = pd.DataFrame ({"Document Number" : highnumbers,
                    "High-Quality Neutral Sentiment": positiveneusentiment})
df2 = pd.DataFrame ({"Document Number" : lownumbers,
                    "Low-Quality Positive Sentiment": negativeneusentiment})


import numpy as np

def loc_eval(x, b):
    loc_est = 0
    for i in enumerate(b): loc_est+=i[1]*(x**i[0])
    return(loc_est)

def loess(xvals, yvals, data, alpha, poly_degree=1):
    all_data = sorted(zip(data[xvals].tolist(), data[yvals].tolist()), key=lambda x: x[0])
    xvals, yvals = zip(*all_data)
    evalDF = pd.DataFrame(columns=['v','g'])
    n = len(xvals)
    m = n + 1
    q = int(np.floor(n * alpha) if alpha <= 1.0 else n)
    avg_interval = ((max(xvals)-min(xvals))/len(xvals))
    v_lb = min(xvals)-(.5*avg_interval)
    v_ub = (max(xvals)+(.5*avg_interval))
    v = enumerate(np.linspace(start=v_lb, stop=v_ub, num=m), start=1)
    xcols = [np.ones_like(xvals)]
    for j in range(1, (poly_degree + 1)):
        xcols.append([i ** j for i in xvals])
    X = np.vstack(xcols).T
    for i in v:
        iterpos = i[0]
        iterval = i[1]
        iterdists = sorted([(j, np.abs(j-iterval)) for j in xvals], key=lambda x: x[1])
        _, raw_dists = zip(*iterdists)
        scale_fact = raw_dists[q-1]
        scaled_dists = [(j[0],(j[1]/scale_fact)) for j in iterdists]
        weights = [(j[0],((1-np.abs(j[1]**3))**3 if j[1]<=1 else 0)) for j in scaled_dists]
        _, weights      = zip(*sorted(weights,     key=lambda x: x[0]))
        _, raw_dists    = zip(*sorted(iterdists,   key=lambda x: x[0]))
        _, scaled_dists = zip(*sorted(scaled_dists,key=lambda x: x[0]))
        W         = np.diag(weights)
        b         = np.linalg.inv(X.T @ W @ X) @ (X.T @ W @ yvals)
        local_est = loc_eval(iterval, b)
        iterDF2   = pd.DataFrame({
                       'v'  :[iterval],
                       'g'  :[local_est]
                       })
        evalDF = pd.concat([evalDF, iterDF2])
    evalDF = evalDF[['v','g']]
    return(evalDF)


evalDF = loess("Document Number", "High-Quality Neutral Sentiment", data = df, alpha=0.7, poly_degree=2)
evalDF3 = loess("Document Number", "Low-Quality Positive Sentiment", data=df2, alpha=0.7, poly_degree=2)


fig = plt.figure()
ax1 = fig.add_subplot(111)
ax1.scatter(df["Document Number"], df["High-Quality Neutral Sentiment"], color="grey", marker="o", s=5, label="_nolegend_")
ax1.plot(evalDF['v'], evalDF['g'], color='red', linewidth= 3, label="High Quality Counselor")
ax1 = fig.add_subplot(111)
ax1.scatter(df2["Document Number"], df2["Low-Quality Positive Sentiment"], color="grey", marker="o", s=5, label="_nolegend_")
ax1.plot(evalDF3['v'], evalDF3['g'], color='turquoise', linewidth= 3, label="Low Quality Counselor")
plt.title('Frequency of Neutral Sentiment')
plt.legend()
plt.tight_layout()
plt.show()

# loess for trendline
import pandas as pd
df = pd.DataFrame ({"Document Number" : highnumbers,
                    "High-Quality Negative Sentiment": positivenegsentiment})
df2 = pd.DataFrame ({"Document Number" : lownumbers,
                    "Low-Quality Negative Sentiment": negativenegsentiment})


import numpy as np

def loc_eval(x, b):
    loc_est = 0
    for i in enumerate(b): loc_est+=i[1]*(x**i[0])
    return(loc_est)

def loess(xvals, yvals, data, alpha, poly_degree=1):
    all_data = sorted(zip(data[xvals].tolist(), data[yvals].tolist()), key=lambda x: x[0])
    xvals, yvals = zip(*all_data)
    evalDF = pd.DataFrame(columns=['v','g'])
    n = len(xvals)
    m = n + 1
    q = int(np.floor(n * alpha) if alpha <= 1.0 else n)
    avg_interval = ((max(xvals)-min(xvals))/len(xvals))
    v_lb = min(xvals)-(.5*avg_interval)
    v_ub = (max(xvals)+(.5*avg_interval))
    v = enumerate(np.linspace(start=v_lb, stop=v_ub, num=m), start=1)
    xcols = [np.ones_like(xvals)]
    for j in range(1, (poly_degree + 1)):
        xcols.append([i ** j for i in xvals])
    X = np.vstack(xcols).T
    for i in v:
        iterpos = i[0]
        iterval = i[1]
        iterdists = sorted([(j, np.abs(j-iterval)) for j in xvals], key=lambda x: x[1])
        _, raw_dists = zip(*iterdists)
        scale_fact = raw_dists[q-1]
        scaled_dists = [(j[0],(j[1]/scale_fact)) for j in iterdists]
        weights = [(j[0],((1-np.abs(j[1]**3))**3 if j[1]<=1 else 0)) for j in scaled_dists]
        _, weights      = zip(*sorted(weights,     key=lambda x: x[0]))
        _, raw_dists    = zip(*sorted(iterdists,   key=lambda x: x[0]))
        _, scaled_dists = zip(*sorted(scaled_dists,key=lambda x: x[0]))
        W         = np.diag(weights)
        b         = np.linalg.inv(X.T @ W @ X) @ (X.T @ W @ yvals)
        local_est = loc_eval(iterval, b)
        iterDF2   = pd.DataFrame({
                       'v'  :[iterval],
                       'g'  :[local_est]
                       })
        evalDF = pd.concat([evalDF, iterDF2])
    evalDF = evalDF[['v','g']]
    return(evalDF)


evalDF = loess("Document Number", "High-Quality Negative Sentiment", data = df, alpha=0.7, poly_degree=2)
evalDF3 = loess("Document Number", "Low-Quality Negative Sentiment", data=df2, alpha=0.7, poly_degree=2)


fig = plt.figure()
ax1 = fig.add_subplot(111)
ax1.scatter(df["Document Number"], df["High-Quality Negative Sentiment"], color="grey", marker="o", s=5, label="_nolegend_")
ax1.plot(evalDF['v'], evalDF['g'], color='red', linewidth= 3, label="High Quality Counselor")
ax1 = fig.add_subplot(111)
ax1.scatter(df2["Document Number"], df2["Low-Quality Negative Sentiment"], color="grey", marker="o", s=5, label="_nolegend_")
ax1.plot(evalDF3['v'], evalDF3['g'], color='turquoise', linewidth= 3, label="Low Quality Counselor")
plt.title('Freqency of Negative Sentiment')
plt.legend()
plt.tight_layout()
plt.show()



"""## Word Ratio"""

wordratio = highratio + lowratio

"""## Word Distance"""

totalworddistance = jpositsimilarity + jnegitsimilarity

"""## LDA Topic Modeling"""

train_vecs = []
from gensim import corpora, models
ID2word = corpora.Dictionary(lemmatizepositive)

train_corpus = [ID2word.doc2bow(doc) for doc in lemmatizepositive]

for i in range(len(lemmatizepositive)):
    top_topics = lda_model.get_document_topics(train_corpus, minimum_probability=0.0)
    topic_vec = [top_topics[i][1] for i in range(7)]
    train_vecs.append(topic_vec)

train_vecs = []
from gensim import corpora, models
ID2word = corpora.Dictionary(lemmatizenegative)

train_corpus = [ID2word.doc2bow(doc) for doc in lemmatizenegative]

for i in range(len(lemmatizenegative)):
    top_topics = lda_model.get_document_topics(train_corpus, minimum_probability=0.0)
    topic_vec = [top_topics[i][1] for i in range(7)]
    train_vecs.append(topic_vec)

positiveldawordvecs = [[(1, 7.807619e-05), (1, 4.5438017e-05), (1, 4.829052e-05), (1, 5.0740815e-05), (1, 4.462692e-05), (1, 6.715556e-06), (1, 3.6889476e-05)]
] * 154
negativeldawordvecs = [[(1, 1.2390344e-05), (1, 0.00012807376), (1, 0.00011617101), (1, 0.000106292515), (1, 0.00011617101), (1, 0.00011483693), (1, 9.252405e-05)]
] * 104

ldawordvecs = positiveldawordvecs + negativeldawordvecs

"""## Labels"""

positivelabels = [1] * 154
negativelabels = [0] * 104
labels = positivelabels + negativelabels

"""## Model"""

import pandas as pd

modelbuilding = {'Document Number': documentnumber,
                 'Word Count': wordcount,
                 'Negative Sentiment Frequency': overallnegative,
                 'Neutral Sentiment Frequency': overallneutral,
                 'Positive Sentiment Frequency': overallpositive,
                 'Counselor-to-Patient Word Ratio': wordratio,
                 'Topics': ldawordvecs,
                 'Labels': labels}

df = pd.DataFrame(modelbuilding, columns = ['Document Number', 'Word Count', 'Negative Sentiment Frequency', 'Neutral Sentiment Frequency', 'Positive Sentiment Frequency', 'Counselor-to-Patient Word Ratio', 'Topics', 'Labels'])

df["Topics"] = pd.to_numeric(df.Topics, errors='coerce')

!pip install pycaret
import pycaret
from pycaret import classification
classification_setup = classification.setup(data= df, target='Labels', numeric_features=['Topics'])
classification.compare_models()

from pycaret.classification import create_model
knn = create_model('knn', fold=3)

from pycaret.classification import save_model
save_model(knn, model_name='knn5')

"""# **App - Streamlit + Pycaret**"""

from pycaret.regression import load_model, predict_model
import streamlit as st
import pandas as pd
import numpy as np

from pycaret.classification import load_model
model = load_model('knn5')

def predict(model, input_df):
  predictions_df = predict_model(estimator=model, data=input_df)
  predictions = predictions_df['Label'][0]
  return predictions

def run():
  from PIL import Image
  image = Image.open('/content/drive/MyDrive/minetlogo.JPG')
  st.image(image,use_column_width = False)
  st.sidebar.info('MINet: A Novel Telemedicine Tool for Automatically Assessing Motivational Interviewing (MI) Conversations Using Natural Language Processing')

# Commented out IPython magic to ensure Python compatibility.
file_upload = st.file_uploader('Upload Transcript for Rating +  Feedback. Ensure they are in PDF/TXT form and th counselor and patient portions are indicated with "c:" and "p:" at the beginning of every line.', type=["txt","pdf"])
if file_upload is not None:
  fu = open(item, 'file_upload')
  file_contents2 = file_upload.read()
  fu.close()
  ru = contractions.fix(file_contents2)
  yu = ru.lower()

  # preprocessing
  import nltk
  nltk.download('punkt')
  nltk.download('stopwords')
  from nltk.tokenize import word_tokenize
  stopwords = nltk.corpus.stopwords.words('english')
  removed = ['c', ':', 't', '.', '?', '!', ',']
  stopwords.extend(removed)

  z = nltk.work_tokenize(yu)
  z2 = [word for word in z if word not in stopwords]

  nltk.download('wordnet')
  from nltk.corpus import wordnet
  nltk.download('averaged_perceptron_tagger')

  def get_wordnet_pos(word):
    tag = nltk.pos_tag([word])[0][1][0].upper()
    tag_dict = {"J": wordnet.ADJ,
                "N": wordnet.NOUN,
                "V": wordnet.VERB,
                "R": wordnet.ADV}

    return tag_dict.get(tag, wordnet.NOUN)

  lemmatizer = WordNetLemmatizer()
  for word in z2:
    qu = ((lemmatizer.lemmatize(word, get_wordnet_pos(word))))

  # word length
  wordcount = len(yu.split())

  # sentiment
  !pip install stanza
  import stanza
  import itertools
  print("Downloading English model...")
  stanza.download('en')
  print("Building an English pipeline...")
  en_nlp = stanza.Pipeline('en')
  lines3 = yu.splitlines()
  for item in lines3:
    sentiment3 = []
    doc = en_nlp(item)
    for i, sentence in enumerate(doc.sentences):
      ruu=sentence.sentiment
      sentiment3.append(ruu)
    sentiment.append(sentiment3)
  from collections import Counter
  sentimentdict = []
  merged = list(itertools.chain(*sentiment))
  a = Counter(merged)
  b= dict(a)
  searchkey1 = 0
  searchkey2 = 1
  searchkey3 = 2
  if searchkey1 in b.keys():
    negativesentiment = ((b[0])/(len(item)))
  else:
    negativesentiment = (0)
  if searchkey2 in b.keys():
    neutralsentiment = ((b[1])/(len(item)))
  else:
    neutralsentiment = (0)
  if searchkey3 in b.keys():
    positivesentiment = ((b[2])/(len(item)))
  else:
    positivesentiment = (0)


  # counselor-patient word ratios
  lines4 = yu.splitlines()
  counselorsq =[]
  patientsq = []
  for item in lines4:
    if item.startsiwth("c: "):
      counselorsq.append(item)
    if item.startswith("p: "):
      patientsq.append(item)
  for item in counselorsq:
    counselorwordcount = len(item.split())
    patientwordcount = len(item.split())
  counselortopatientratio = counselorwordcount/patientwordcount

  # jaccard similarity
  def get_jaccard_sim(str1, str2):
    a = set(str1.split())
    b = set(str2.split())
    c = a.intersection(b)
    return float(len(c)) / (len(a) + len(b) - len(c))

  for item in counselorsq:
    counselor = item
  for item in patientsq:
    patient = item
  jaccardsim = get_jaccard_sim(counselor, patient)

  # lda topic vectors
  topicqu = []
  topicqu.append(qu)
  train_vecs = []
  from gensim import corpora, models
  dictionary_LDA3 = corpora.Dictionary(topicqu)
  corpus = [dictionary_LDA3.doc2bow(list_of_tokens) for list_of_tokens in topicqu]
  num_topics = 5
#   %time lda_model3 = models.LdaModel(corpus, num_topics=num_topics, \
                                  id2word=dictionary_LDA3, \
                                  passes=4, alpha=[0.01]*num_topics, \
                                  eta=[0.01]*len(dictionary_LDA3.keys()))

  from gensim.models import CoherenceModel
  # Compute Coherence Score
  coherence_model_lda3 = CoherenceModel(model=lda_model3, texts=topicqu, dictionary=dictionary_LDA3, coherence='c_v')
  coherence_lda3 = coherence_model_lda3.get_coherence()

  train_vecs = []
  from gensim import corpora, models
  ID2word4 = corpora.Dictionary(topicqu)

  train_corpus = [ID2word4.doc2bow(doc) for doc in topicqu]

  for i in range(len(topicqu)):
    top_topics4 = lda_model3.get_document_topics(train_corpus4, minimum_probability=0.0)
    topic_vec4 = [top_topics[i][1] for i in range(7)]
    train_vecs4.append(topic_vec4 )

yu2 = '''c: Zendaya Maree Stoermer Coleman is an American actress, singer and producer.
p: She began her career as a child model and backup dancer, before gaining prominence for her role as Rocky Blue on the Disney Channel sitcom Shake It Up'''
!pip install stanza
import stanza
import itertools
print("Downloading English model...")
stanza.download('en')
print("Building an English pipeline...")
en_nlp = stanza.Pipeline('en')
lines3 = yu2.splitlines()

sentiment = []
for item in lines3:
  sentiment3 = []
  doc = en_nlp(item)
  for i, sentence in enumerate(doc.sentences):
    ruu=sentence.sentiment
    sentiment3.append(ruu)
  sentiment.append(sentiment3)
from collections import Counter
sentimentdict = []

merged = list(itertools.chain(*sentiment))

a = Counter(merged)
b= dict(a)
searchkey1 = 0
searchkey2 = 1
searchkey3 = 2
if searchkey1 in b.keys():
  negativesentiment = ((b[0])/(len(item)))
else:
  negativesentiment = (0)
if searchkey2 in b.keys():
  neutralsentiment = ((b[1])/(len(item)))
else:
  neutralsentiment = (0)
if searchkey3 in b.keys():
  positivesentiment = ((b[2])/(len(item)))
else:
  positivesentiment = (0)

print(b)
print(neutralsentiment)
print(positivesentiment)
print(negativesentiment)



Accuracy  AUC	  Recall	 Prec.	  F1	   Kappa	 MCC
0.9000	0.9408	0.9249	0.9098	0.9166	0.7918	0.7937
0.9056	0.9576	0.9277	0.9202	0.9237	0.7997	0.8005
0.9111	0.9589	0.8980	0.9365	0.9167	0.8216	0.8228

